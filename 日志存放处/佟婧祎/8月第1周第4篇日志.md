## 交叉熵损失函数

![img](https://latex.csdn.net/eq?L_%7Bi%7D%3D-log%28%5Cfrac%7Be%5E%7Bsy_%7Bi%7D%7D%7D%7B%5Csum%20_%7Bj%7De%5E%7Bsj%7D%7D%29) 

极大似然：正确分类发生的联合概率，

​               属于各自正确类别的概率相乘，就能得到所有图片都分类正确的概率。

​               因为结果非常小所以在外面求一个对数

括号里：图像属于正确分类的概率越接近1，交叉熵损失函数值就越接近0

只关心图片属于正确分类的概率

最小值是0，最大值正无穷

### 优化方法

跟随梯度：不是随机寻找方向，直接计算（好比一个下山的过程）

数值梯度法：是借助于梯度的定义对其进行逼近，加一个非常小的数（较慢，得出近似值）

解析梯度法：用微积分，得到梯度公式（速度快，结果不是近似值，但易出错，常用）

### 梯度下降

1.普通梯度下降：梯度下降是对神经网络的损失函数最优化中最常用的方法

2.小批量：每一次256个，求256个的损失函数值

```python
while True:

  data_batch = sample_training_data(data, 256) # 从大规模训练样本中提取256个样本

  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)

  weights += - step_size * weights_grad # 参数更新
```



#### 总结

使用铰链损失函数可以计算各类别的分数和真实标签之间的距离，计算铰链损失函数值。

经过 `Softmax` 函数变换后，可以计算交叉熵损失函数。

为了使模型更加简单可以在损失函数后面加正则化。