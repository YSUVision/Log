机器学习

# 一学习任务

## 1监督学习的了解

### 监督学习具有以下特征：

有明确的目标

- 存在已知的目标变量（或标签），学习的目的是建立输入特征与目标变量之间的映射关系。

数据标注

- 训练数据集中的每个样本都有对应的正确标签或目标值，用于指导模型的学习过程。

可评估性

- 可以使用各种指标（如准确率、召回率、均方误差等）来评估模型的性能和效果。

泛化能力

- 期望模型能够对未见过的新数据做出准确的预测，具有良好的泛化能力。

### 监督学习主要分为两类：回归和分类。

**回归**：

- 目标是预测一个连续的值。
- 例如，预测房价、股票价格、气温等。
- 常见的回归算法包括线性回归、多项式回归、岭回归、Lasso 回归等。

**分类**：

- 目的是将数据划分到不同的类别中。
- 比如，判断邮件是否为垃圾邮件（是/否）、识别手写数字（0-9）、预测患者是否患有某种疾病（患病/未患病）。
- 常见的分类算法有逻辑回归、决策树、随机森林、支持向量机、朴素贝叶斯等。

在实际应用中，需要根据具体的问题和数据特点来选择使用回归还是分类算法。例如，如果要预测一个具体的数值，通常会选择回归；而如果要确定数据属于哪个类别，就会选择分类算法。

## 2无监督学习的了解

相比于监督学习 没有目标标签 所以需要自己找到感兴趣区域   没告诉机器而是让机器自己去分别出标签

比如

聚类算法  ：获取没有标签的数据 并尝试自动将他们分组到集群中   将相似的数据点组合在一起

无监督学习的特征

### **无监督学习具有以下几个主要特征**：

1. **无需人工标记数据**：在无监督学习中，数据没有预先被赋予明确的类别或标签，算法需要自行从数据中发现模式和结构。
   - 例如，对于大量的文本数据，算法能够自动识别出相似的文本段落，而不需要事先知道这些段落属于何种具体类别。
2. **发现隐藏模式**：旨在揭示数据中潜在的、隐藏的结构和模式。
   - 比如通过聚类算法，将具有相似特征的数据点聚集在一起，形成不同的簇，这些簇可能代表了数据中自然存在的分组。
3. **数据驱动**：完全依赖数据本身的特征和分布来进行学习和分析。
   - 以图像数据为例，无监督学习可以发现图像中相似的颜色、形状或纹理模式。
4. **可扩展性强**：能够处理大规模的数据集，因为不需要人工进行大量的标注工作。
   - 像处理海量的用户行为数据，挖掘其中的潜在规律。
5. **探索性分析**：帮助人们更好地理解数据的内在结构和特征。
   - 例如，分析客户的购买行为数据，以发现一些未曾预料到的消费模式。



### 无监督学习的分类

1. **聚类算法**：
   - K-Means 聚类：通过将数据点划分到不同的簇中，使得簇内的数据点相似度较高，而簇间的相似度较低。
   - 层次聚类：可以构建树形的聚类结构，有自底向上的聚合层次聚类和自顶向下的分裂层次聚类两种方式。
   - 密度聚类：如 DBSCAN 算法，基于数据点的密度来发现簇。
2. **降维算法**：
   - 主成分分析（PCA）：将高维数据投影到低维空间，同时保留数据的主要特征。
   - 奇异值分解（SVD）：常用于数据压缩和特征提取。
3. **关联规则学习**：
   - Apriori 算法：用于挖掘数据集中频繁出现的项集以及它们之间的关联规则。
4. **生成模型**：
   - 自编码器（Autoencoder）：用于数据的压缩和特征提取，能够学习数据的潜在表示。
5. **异常检测算法**：
   - 基于统计的方法：通过设定阈值来判断数据点是否为异常。
   - 基于距离的方法：如 K 近邻算法，根据数据点与周围邻居的距离来判断是否异常。

## 3 Jupyter Notebook 

。。。

## 4 线性回归模型的学习（未学完）



**监督学习中的线性回归模型知识点总结如下**：

1. **基本概念**

   - 线性回归是用于预测连续数值型目标变量的监督学习算法。
   - 它假设目标变量与输入特征之间存在线性关系。

2. **模型表达式**

   - 一般形式为 `y = w₁x₁ + w₂x₂ +... + wₙxₙ + b` ，其中 `y` 是预测值，`x₁, x₂,..., xₙ` 是特征，`w₁, w₂,..., wₙ` 是权重，`b` 是偏置项。

3. **成本函数**

   它用于衡量模型在训练数据上的预测值与真实值之间的不一致程度。简单来说，成本函数的值越小，模型的性能就越好。

   - 常用的是均方误差，即 `L(w, b) = 1/2m ∑(yᵢ - ŷᵢ)²` ，其中 `m` 是样本数量，`yᵢ` 是真实值，`ŷᵢ` 是预测值。

- 适用场景：常用于线性回归问题。
- 特点：对较大的误差给予更高的惩罚权重，因为误差是平方的。
- 例如，预测房价时，如果预测值与真实值相差较大，MSE 会给出较大的惩罚。



4 优化算法的梯度下降法

  **梯度下降法** 是机器学习和深度学习中用于优化模型参数以最小化损失函数的一种常用算法。

**原理**：
成本函数通常是关于模型参数（如权重和偏置项）的函数。梯度下降法的核心思想是沿着损失函数的负梯度方向更新参数，因为负梯度方向是函数下降最快的方向。

**具体步骤**：

1. 初始化参数：随机或根据经验给定模型参数的初始值。
2. 计算梯度：计算当前参数下损失函数关于每个参数的梯度。
3. 更新参数：按照以下公式更新参数：`θ = θ - η * ∇J(θ)` ，其中 `θ` 是参数，`η` 是学习率，`∇J(θ)` 是损失函数 `J(θ)` 的梯度。

**学习率（Learning Rate）**：
学习率决定了每次参数更新的步长。如果学习率过大，可能会导致参数在最优值附近来回震荡，无法收敛；如果学习率过小，会使收敛速度非常缓慢。



# 二完成情况

   目前入门中 还有需要需要学

# 三学习中存在的问题

Jupyter Notebook 有没有安装链接啥的  登陆后怎么使用

# 四 总结和思考

   机器学习刚开始入门，还需要学的很多